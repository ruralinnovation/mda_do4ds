[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MDA DevOps for Data Science",
    "section": "",
    "text": "Preface\nThe MDA team at CORI will do a book club and review the book DevOps for Data Science written by Alex K Gold here.\n\nBook study repo\nBook study site\nBook study google drive\n\n\n\nPlanning:\n\nIntroduction + Environments as Code: Olivier\nData Project Architecture: John\nDatabases and Data APIs: Brittany\nLogging and Monitoring: John\nDeployments and Code Promotion: Camden\nDemystifying Docker: John\n…\n\n\n\nWorkflow:\n\nClone this repo\nCreate a specific branch:\n\n  git checkout -b ch-?/???\n  # (replace `?` with the chapter number and `???` with a name/title)\n\nInstall Quarto CLI\nInstall local dependencies:\n\n  npm install\n\nPreview Quarto site:\n\n  npm run preview\n\nMake changes and commit as needed\nPush commits and create a pull request\n\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Definitions\nproduction :\nWe want:",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/intro.html#definitions",
    "href": "chapters/intro.html#definitions",
    "title": "Introduction",
    "section": "",
    "text": "affecting decision in your orgs / world\n\n\nputting your work in front of someone else’s eyes\n\n\n\nour works to be reliable\nin a safe environment\nour work to be available",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/intro.html#devops",
    "href": "chapters/intro.html#devops",
    "title": "Introduction",
    "section": "DevOps",
    "text": "DevOps\n\nDevOps is a set of cultural norms, practices, and tooling to help make developing and deploying software smoother and lower risk.\n\n.. but is a squishy concept and a “vendor” associated name\nIt came in opposition to the waterfall dev process were you had a team doing Dev. and then one other doing Ops (Ops “make it works on everyone computer”).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/intro.html#process-and-people",
    "href": "chapters/intro.html#process-and-people",
    "title": "Introduction",
    "section": "Process and people",
    "text": "Process and people\n\nAre data scientist software developper?\nAre we in the red flags number 3?\nDo we need a workbench (ie using ec2) ?\nshould we do the exercice with penguins or one of our dataset?",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html",
    "href": "chapters/sec1/1-0-sec-intro.html",
    "title": "DevOps Lessons for Data Science",
    "section": "",
    "text": "You are a software developer.\n\nBut:\n\nWriting code for data science is different than writing code:\n\n\nYou’re pointed at some data and asked to derive value from it without even knowing if that’s possible.\n\n\ndifference between architect and archaeologist",
    "crumbs": [
      "DevOps Lessons for Data Science"
    ]
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html",
    "href": "chapters/sec1/1-1-env-as-code.html",
    "title": "1  Environments as Code",
    "section": "",
    "text": "1.1 Environments:\nBuilding a completly reproducible environement is a “fool’s errand” but first step should be easy.\n(any trouble with renv and sf anyone?)",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Environments as Code</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#environments",
    "href": "chapters/sec1/1-1-env-as-code.html#environments",
    "title": "1  Environments as Code",
    "section": "",
    "text": "stack of software and hardware below our code\nshould be treated as “cattle not pet” / should be stateless\nRisk of it “only works on my machine”",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Environments as Code</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#environements-have-layers",
    "href": "chapters/sec1/1-1-env-as-code.html#environements-have-layers",
    "title": "1  Environments as Code",
    "section": "1.2 Environements have layers",
    "text": "1.2 Environements have layers\n\n\n\nLayer\nContents\nExample\n\n\n\n\nPackages\nR packages\ncori.db\n\n\nSystem\nR versions / GDAL / MacOS\n14.4.1\n\n\nhardware\nPhysical / Virtual hardware\nApple M3\n\n\n\nHardware and System should be in the hand of IT (see later chapter 7 and 14), packages layer should be the data scientist.",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Environments as Code</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#the-package-layer",
    "href": "chapters/sec1/1-1-env-as-code.html#the-package-layer",
    "title": "1  Environments as Code",
    "section": "1.3 The package layer",
    "text": "1.3 The package layer\nPackage can in 3 places:\n\nrepository: CRAN / GH / “Supermarket”\nlibrary: a folder on a drive / “pantry”\nloaded : “ready to cook”\n\nEach project should have it’s own “pantry”\nProject was higlighted in text but I think it is important: if you do not have a project workflow it is way harder to do it.\nA package environement shouldbe :\n\nisolated and cannot be disrupted (example updating a packge in an other project)\ncan be “captured” and “transported”\n\nIn R: {Renv} (“light”/“not exactly the same” option also exist, Box, capsule)\nAuthor does not like Conda (good to not being alone!)",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Environments as Code</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#workflow",
    "href": "chapters/sec1/1-1-env-as-code.html#workflow",
    "title": "1  Environments as Code",
    "section": "1.4 Workflow",
    "text": "1.4 Workflow\n\nCreate a standalone directory with a virtual environment\n\n(spend time exploring renv/ and .gitignore)\n\nDocument environment state (see lockfile)\nCollaborate / deploy: you can’t share package because their binay can be OS or system specific, hence specific package need to be installed (could be a pain point).\nUse virtual env",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Environments as Code</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#under-the-hood",
    "href": "chapters/sec1/1-1-env-as-code.html#under-the-hood",
    "title": "1  Environments as Code",
    "section": "1.5 Under the hood",
    "text": "1.5 Under the hood\n\ntest .libpaths() in a specific project and in a “random” R session\norder of Paths matter",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Environments as Code</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#key-points",
    "href": "chapters/sec1/1-1-env-as-code.html#key-points",
    "title": "1  Environments as Code",
    "section": "1.6 Key points",
    "text": "1.6 Key points\n\nbeing in production is what make a DS a software developper\nkill and create new environment fast is important",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Environments as Code</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html",
    "href": "chapters/sec1/1-2-proj-arch.html",
    "title": "2  Data Project Architecture",
    "section": "",
    "text": "2.1 Key Takeaways\nThis chapter gives an opinionated overview of good design and conceptual layout practices in regards to a data project. The areas of responsibility within the project are broken out into\nThe categories that a given data project may fall into our further divided into\nThe rest of the chapter discusses how to break a project down into the previously mentioned layers, as well as considerations for optimizing the Processing and Data layers.",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Project Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#key-takeaways",
    "href": "chapters/sec1/1-2-proj-arch.html#key-takeaways",
    "title": "2  Data Project Architecture",
    "section": "",
    "text": "Presentation,\nProcessing, and\nData layers.\n\n\n\njobs,\napps,\nreports and\nAPI’s.",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Project Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#lab-project",
    "href": "chapters/sec1/1-2-proj-arch.html#lab-project",
    "title": "2  Data Project Architecture",
    "section": "2.2 Lab / Project",
    "text": "2.2 Lab / Project\n\n2.2.1 Initial Setup\nThe last chapter, Environments as Code, introduced the example project that we will use throughout the book. You can either clone a starter template for fork the project repo from do4ds_project or create the project from scratch yourself using the following Quarto CLI commands (taken from the Quarto documentation):\nquarto create project website do4ds_project\n# Choose (don't open) when prompted\n\nquarto preview do4ds_project\n… if the quarto preview command loads a new website in your web browser, go back to the terminal and use Ctrl+C to terminate the preview server. Change to the project directory and setup a local python virtual environment (you can grab the requirements.txt file from here, if needed):\ncd do4ds_project\n# If using python, create and activate a local virtual environment\npython -m venv ./venv\nsource venv/bin/activate\nvenv/bin/python -m pip install -r requirements.txt\nNow that you are in the local project directory you can use the quarto preview command without arguments to continue seeing updates to the local project in your browser:\nquarto preview\n\n# Alternately, if you forked the project sample from Github, you can use npm...\nnpm run preview\nIf you did not fork the project sample, make sure to create the eda.qmd and model.qmd files from chapter 1 and add them to the sidebar section of _quarto.yml:\nproject:\n  type: website\n\nwebsite:\n  title: \"do4ds_project\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n  sidebar:\n    style: \"docked\"\n    search: true\n    contents:\n      - eda.qmd\n      - model.qmd\n\n\n\n\n2.2.2 Updates\nTo complete part 1 of the lab, I had to modify the example code. First, I added a line that would generate a vetiver model and assign it to v and then I changed the path to the local folder where the model could be stored:\nfrom pins import board_folder\nfrom vetiver import vetiver_pin_write\nfrom vetiver import VetiverModel\n\nv = VetiverModel(model, model_name = \"penguin_model\")\n\nmodel_board = board_folder(\n  \"data/model\",\n  allow_pickle_read = True\n)\nvetiver_pin_write(model_board, v)\nIn addition to these changes, I created a separate Python file with the code to run the vetiver API, called api.py, which also required updates to the VetiverApi call to ensure that the API server had the correct input params in order to process the prediction:\nfrom palmerpenguins import penguins\nfrom pandas import get_dummies\nfrom sklearn.linear_model import LinearRegression\nfrom pins import board_folder\nfrom vetiver import VetiverModel\nfrom vetiver import VetiverAPI\n\n# This is how you would reload the model from disk...\nb = board_folder('data/model', allow_pickle_read = True)\nv = VetiverModel.from_pin(b, 'penguin_model')\n\n# ... however VertiverAPI also uses the model inputs to define params from the prototype\ndf = penguins.load_penguins().dropna()\ndf.head(3)\nX = get_dummies(df[['bill_length_mm', 'species', 'sex']], drop_first = True)\ny = df['body_mass_g']\n\nmodel = LinearRegression().fit(X, y)\n\nv = VetiverModel(model, model_name = \"penguin_model\", prototype_data = X)\n\napp = VetiverAPI(v, check_prototype = True)\napp.run(port = 8000)\n… and then used python api.py to run the API. Once running, you can navigate to http://127.0.0.1:8000/docs in a web browser to see the autogenerated API documentation",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Project Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html",
    "href": "chapters/sec1/1-3-data-access.html",
    "title": "3  Databases and Data APIs",
    "section": "",
    "text": "3.1 Key Takeaways\nAlthough it’s not explicitly described in this chapter, a database is, at minimum, two things:\nIn most programming languages, the way you begin an interaction with a database is to create a connection object. The methods and params used to create that object will vary from database type to type and from language to language, including how credentials are provided or set, but the successful creation of this object implies that you have an open connection to the database with a specific set of permissions (read/write/remove, etc.).\nAnother source of data may be an API, or Application Programming Interface. The term “API” is actually used in this chapter in two different contexts; the first is to describe the method (or interface) used in particular programming languages to connect to a database, but in the second usage the context is REST or REST-ful API’s. These are collections of URL’s that allow a server to listen for requests for data from clients. Once a client makes a request to a given URL with specified parameters, the server evaluates the request, computes a response and returns the results. Although this is very similar to how a database operates, a REST-ful API uses a specific protocol called the HyperText Transfer Protocol (HTTP). This protocol uses the parts of the URL (i.e., the path and the query string) and the verb of the client request (i.e., GET, POST, PUT, DELETE, etc.) as a way to specify how the server should evaluate the request. This protocol is widely available in many applications, including those that do no support any database drivers (like web browsers). Each programming languages has specific packages or libraries for making HTTP requests, as well as packages that provide “wrapper” functions; functions that hide the details of each API request from the developer and simply take arguments and return results in a synchronous manner. In addition to parameters, a REST-ful API endpoint may accept a body (block of code or text) as part of the request, usually in the form of a JSON object, which is often also the preferred format of the response body (again, block of code or text) chosen by API developers. There are various ways to include credentials with HTTP requests.\nWhile I believe the topic should have been introduced earlier in this book, this chapter also discusses environment variables and how they can be used to provide configuration settings to the process that the code is running within, including database or API credentials.\nAt CORI/RISI we have our own “data connector” package called cori.db, which allows each of us to connect directly to the database and interact with it in a prescribed way. At the moment, we do not have an equivalent set of function to interact with the CORI Data API, although we have created a placeholder package called cori.data.api which may one day provide such functions in R.",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Databases and Data APIs</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#key-takeaways",
    "href": "chapters/sec1/1-3-data-access.html#key-takeaways",
    "title": "3  Databases and Data APIs",
    "section": "",
    "text": "a datastore (i.e., a way to store structured data) and\na compute engine (i.e., a way to perform operations on structured data).",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Databases and Data APIs</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-3-data-access.html#lab-project",
    "href": "chapters/sec1/1-3-data-access.html#lab-project",
    "title": "3  Databases and Data APIs",
    "section": "3.2 Lab / Project",
    "text": "3.2 Lab / Project\n\n3.2.1 Updates\nThe lab work introduces the duckdb package, so I created a new markdown file called “00_data.qmd” in the project sample that demonstrates the most basic syntax for creating a new database (which is stored in a flat file on local disk) and loading the penguins data into a new table in that database. To make sure that this file is processed before the others, I prefixed the names of all the quarto files with sequences numbers (“00_…”, “01_…”, etc.). The model and api code has been updated to retrieve the penguin table from the local database store with duckdb. Olivier added an “02_model_R.qmd” file to demonstrate the R code that is equivalent to the modeling example used in the book. The same could be done for the vetiver code used to deploy the resulting model as an API endpoint.",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Databases and Data APIs</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html",
    "href": "chapters/sec1/1-4-monitor-log.html",
    "title": "4  Logging and Monitoring",
    "section": "",
    "text": "4.1 Key Takeaways\nIn any data processing and/or analysis, something is going to go wrong.\nData science (and by extension programmatic data analysis) is hard, because the output is “novel”, so we should “use process metrics to reveal a problem before it surfaces in your results”.\nThe easiest level to observe or monitor process metrics is at the Data and Processing layers (why?).\nThe author believes that all job-type projects should be encapsulated in a literate programming format (i.e. Quarto markdown), so that the output (log) of the process is combined with notes about its context. Is this an evolution over comments in the code?\nThings to check when transforming data:\nFor the purposes of logging, print is a good start, but consistently using a logging package is better. The author recommendslog4r. With this package, the output (like most conventional log outputs) has 3 components:\nLog level commonly uses the following ranking:\nLog messages are often formatted as either plain text of JSON. Log output defaults to the current terminal, but can be directed to a file on local disk, a remote store like S3, or to a logging API like AWS CloudWatch.",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Logging and Monitoring</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#key-takeaways",
    "href": "chapters/sec1/1-4-monitor-log.html#key-takeaways",
    "title": "4  Logging and Monitoring",
    "section": "",
    "text": "Process metrics - quantitative and qualitative measures related to a process, its performance and its evolution.\n\n\n\n\n\nQuality of each join (row count)\nCross-tabulation before and after recoding (dplyr::mutate)\nFitness of model (i.e. bias, variance, drift, etc.)\n\n\n\nlog metadata\nlog level\nlog data\n\n\n\nDEBUG\nINFO\nWARN (or WARNING)\nERROR\nCRITICAL",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Logging and Monitoring</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-4-monitor-log.html#lab-project",
    "href": "chapters/sec1/1-4-monitor-log.html#lab-project",
    "title": "4  Logging and Monitoring",
    "section": "4.2 Lab / Project",
    "text": "4.2 Lab / Project\n\n4.2.1 Updates\nWe started logging messages and errors in the api.py and app.R scripts, with the respective logging and log4r packages.",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Logging and Monitoring</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html",
    "href": "chapters/sec1/1-5-deployments.html",
    "title": "5  Chapter 5 Notes",
    "section": "",
    "text": "5.1 Key Takeaways\nBenefits of CI/CD: - Ability to build your code from scratch - Source code is always available in a central repository - Automated testing and deployment simplifies and standardizes the testing/deployment process\nTactics: - Separate the dev and prod environments - Changes only happen through a promotion process from dev (through test) and into prod - Code promotion happens via Version Control (e.g., git)\nAuthor’s suggestion for managing data science projects: 1. Maintain two long-running branches – main is the prod version of your project and test is a long-running pre-prod version. 2. Code can only be promoted to main via a merge from test. Direct pushes to main are not allowed. 3. New functionality is developed in short-lived feature branches that are merged into test when you think they’re ready to go. Once sufficient approvals are granted, the feature branch changes in test are merged into main.\nOther: - Use YAML for conditional environments\n…",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5 Notes</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-5-deployments.html#lab-project",
    "href": "chapters/sec1/1-5-deployments.html#lab-project",
    "title": "5  Chapter 5 Notes",
    "section": "5.2 Lab / Project",
    "text": "5.2 Lab / Project\n\n5.2.1 Updates\n…",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chapter 5 Notes</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html",
    "href": "chapters/sec1/1-6-docker.html",
    "title": "6  Key Takeaways",
    "section": "",
    "text": "6.1 Lab / Project",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Key Takeaways</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-6-docker.html#lab-project",
    "href": "chapters/sec1/1-6-docker.html#lab-project",
    "title": "6  Key Takeaways",
    "section": "",
    "text": "6.1.1 Updates\n…",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Key Takeaways</span>"
    ]
  }
]