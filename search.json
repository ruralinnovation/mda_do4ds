[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MDA DevOps for Data Science",
    "section": "",
    "text": "Preface\nThe MDA team at CORI will do a book club and review the book DevOps for Data Science writen by Alex K Gold here.\n\nBook study repo\nBook study site\nBook study google drive\n\n\n\nPlanning:\n\nIntroduction + Environments as Code: Olivier\nData Project Architecture: John\nDatabases and Data APIs: Brittany\nLogging and Monitoring: Dolley\nDeployments and Code Promotion: Camden\nDemystifying Docker: John\n…\n\n\n\nWorkflow:\n\nClone this repo\nCreate a specific branch:\n\n  git checkout -b ch-?/???\n  # (replace `?` with the chapter number and `???` with a name/title)\n\nInstall Quarto CLI\nInstall local dependencies:\n\n  npm install\n\nPreview Quarto site:\n\n  npm run preview\n\nMake changes and commit as needed\nPush commits and create a pull request\n\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Definitions\nproduction :\nWe want:",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/intro.html#definitions",
    "href": "chapters/intro.html#definitions",
    "title": "Introduction",
    "section": "",
    "text": "affecting decision in your orgs / world\n\n\nputting your work in front of someone else’s eyes\n\n\n\nour works to be reliable\nin a safe environment\nour work to be available",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/intro.html#devops",
    "href": "chapters/intro.html#devops",
    "title": "Introduction",
    "section": "DevOps",
    "text": "DevOps\n\nDevOps is a set of cultural norms, practices, and tooling to help make developing and deploying software smoother and lower risk.\n\n.. but is a squishy concept and a “vendor” associated name\nIt came in opposition to the waterfall dev process were you had a team doing Dev. and then one other doing Ops (Ops “make it works on everyone computer”).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/intro.html#process-and-people",
    "href": "chapters/intro.html#process-and-people",
    "title": "Introduction",
    "section": "Process and people",
    "text": "Process and people\n\nAre data scientist software developper?\nAre we in the red flags number 3?\nDo we need a workbench (ie using ec2) ?\nshould we do the exercice with penguins or one of our dataset?",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/sec1/1-0-sec-intro.html",
    "href": "chapters/sec1/1-0-sec-intro.html",
    "title": "DevOps Lessons for Data Science",
    "section": "",
    "text": "You are a software developer.\n\nBut:\n\nWriting code for data science is different than writing code:\n\n\nYou’re pointed at some data and asked to derive value from it without even knowing if that’s possible.\n\n\ndifference between architect and archaeologist",
    "crumbs": [
      "DevOps Lessons for Data Science"
    ]
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html",
    "href": "chapters/sec1/1-1-env-as-code.html",
    "title": "1  Environments as Code",
    "section": "",
    "text": "1.1 Environments:\nBuilding a completly reproducible environement is a “fool’s errand” but first step should be easy.\n(any trouble with renv and sf anyone?)",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Environments as Code</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#environments",
    "href": "chapters/sec1/1-1-env-as-code.html#environments",
    "title": "1  Environments as Code",
    "section": "",
    "text": "stack of software and hardware below our code\nshould be treated as “cattle not pet” / should be stateless\nRisk of it “only works on my machine”",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Environments as Code</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#environements-have-layers",
    "href": "chapters/sec1/1-1-env-as-code.html#environements-have-layers",
    "title": "1  Environments as Code",
    "section": "1.2 Environements have layers",
    "text": "1.2 Environements have layers\n\n\n\nLayer\nContents\nExample\n\n\n\n\nPackages\nR packages\ncori.db\n\n\nSystem\nR versions / GDAL / MacOS\n14.4.1\n\n\nhardware\nPhysical / Virtual hardware\nApple M3\n\n\n\nHardware and System should be in the hand of IT (see later chapter 7 and 14), packages layer should be the data scientist.",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Environments as Code</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#the-package-layer",
    "href": "chapters/sec1/1-1-env-as-code.html#the-package-layer",
    "title": "1  Environments as Code",
    "section": "1.3 The package layer",
    "text": "1.3 The package layer\nPackage can in 3 places:\n\nrepository: CRAN / GH / “Supermarket”\nlibrary: a folder on a drive / “pantry”\nloaded : “ready to cook”\n\nEach project should have it’s own “pantry”\nProject was higlighted in text but I think it is important: if you do not have a project workflow it is way harder to do it.\nA package environement shouldbe :\n\nisolated and cannot be disrupted (example updating a packge in an other project)\ncan be “captured” and “transported”\n\nIn R: {Renv} (“light”/“not exactly the same” option also exist, Box, capsule)\nAuthor does not like Conda (good to not being alone!)",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Environments as Code</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#workflow",
    "href": "chapters/sec1/1-1-env-as-code.html#workflow",
    "title": "1  Environments as Code",
    "section": "1.4 Workflow",
    "text": "1.4 Workflow\n\nCreate a standalone directory with a virtual environment\n\n(spend time exploring renv/ and .gitignore)\n\nDocument environment state (see lockfile)\nCollaborate / deploy: you can’t share package because their binay can be OS or system specific, hence specific package need to be installed (could be a pain point).\nUse virtual env",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Environments as Code</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#under-the-hood",
    "href": "chapters/sec1/1-1-env-as-code.html#under-the-hood",
    "title": "1  Environments as Code",
    "section": "1.5 Under the hood",
    "text": "1.5 Under the hood\n\ntest .libpaths() in a specific project and in a “random” R session\norder of Paths matter",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Environments as Code</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-1-env-as-code.html#key-points",
    "href": "chapters/sec1/1-1-env-as-code.html#key-points",
    "title": "1  Environments as Code",
    "section": "1.6 Key points",
    "text": "1.6 Key points\n\nbeing in production is what make a DS a software developper\nkill and create new environment fast is important",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Environments as Code</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html",
    "href": "chapters/sec1/1-2-proj-arch.html",
    "title": "2  Data Project Architecture",
    "section": "",
    "text": "2.1 Initial project setup\nThe last chapter, Environments as Code, introduced the example project that we will use throughout the book. You can either clone a starter template for this project from do4ds_project or create the project from scratch yourself using the following Quarto CLI commands (taken from the Quarto documentation):\n… if the quarto preview command loads a new website in your web browser, go back to the terminal and use Ctrl+C to terminate the preview server. Change to the project directory and setup a local python virtual environment (if needed, you can grab the requirements.txt file from here):\nNow that you are in the local project directory you can use the quarto preview command without arguments to continue seeing updates to the local project in your browser:\nIf you did not use the project template, make sure to create the eda.qmd and model.qmd files from chapter 1 and add them to the sidebar section of _quarto.yml:",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Project Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#initial-project-setup",
    "href": "chapters/sec1/1-2-proj-arch.html#initial-project-setup",
    "title": "2  Data Project Architecture",
    "section": "",
    "text": "quarto create project website do4ds_project\n# Choose (don't open) when prompted\n\nquarto preview do4ds_project\n\ncd do4ds_project\n# If using python, create and activate a local virtual environment\npython -m venv ./venv\nsource venv/bin/activate\nvenv/bin/python -m pip install -r requirements.txt\n\nquarto preview\n\n# Alternately, if you copied the project template from Github, you can use npm...\nnpm run preview\n\nproject:\n  type: website\n\nwebsite:\n  title: \"do4ds_project\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n  sidebar:\n    style: \"docked\"\n    search: true\n    contents:\n      - eda.qmd\n      - model.qmd",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Project Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#key-takeaways",
    "href": "chapters/sec1/1-2-proj-arch.html#key-takeaways",
    "title": "2  Data Project Architecture",
    "section": "2.2 Key Takeaways",
    "text": "2.2 Key Takeaways\nThis chapter gives an opiniated overview of good design and conceptual layout practices in regards to a data project. The areas of responsibility within the project our broken out into 1) Presentation, 2) Processing, and 3) Data layers. The categories that a given data project may fall into our further divided into 1) jobs, 2) apps, 3) reports and 4) API’s. The rest of the chapter discusses how to break a project down into the previously mentioned layers, as well as considerations for optimizing the Processing and Data layers.",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Project Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/sec1/1-2-proj-arch.html#lab",
    "href": "chapters/sec1/1-2-proj-arch.html#lab",
    "title": "2  Data Project Architecture",
    "section": "2.3 Lab",
    "text": "2.3 Lab\nTo complete part 1 of the lab, I had to modify the example code. First, I added a line that would generate a vetiver model and assign it to v and then I changed the path to the local folder where the model could be stored:\nfrom pins import board_folder\nfrom vetiver import vetiver_pin_write\nfrom vetiver import VetiverModel\n\nv = VetiverModel(model, model_name = \"penguin_model\")\n\nmodel_board = board_folder(\n  \"data/model\",\n  allow_pickle_read = True\n)\nvetiver_pin_write(model_board, v)\nIn addition to these changes, I created a separate Python file with the code to run the vetiver API, called api.py, which also required updates to the VetiverApi call to ensure that the API server had the correct input params in order to process the prediction:\nfrom palmerpenguins import penguins\nfrom pandas import get_dummies\nfrom sklearn.linear_model import LinearRegression\nfrom pins import board_folder\nfrom vetiver import VetiverModel\nfrom vetiver import VetiverAPI\n\n# This is how you would reload the model from disk...\nb = board_folder('data/model', allow_pickle_read = True)\nv = VetiverModel.from_pin(b, 'penguin_model')\n\n# ... however VertiverAPI also uses the model inputs to define params from the prototype\ndf = penguins.load_penguins().dropna()\ndf.head(3)\nX = get_dummies(df[['bill_length_mm', 'species', 'sex']], drop_first = True)\ny = df['body_mass_g']\n\nmodel = LinearRegression().fit(X, y)\n\nv = VetiverModel(model, model_name = \"penguin_model\", prototype_data = X)\n\napp = VetiverAPI(v, check_prototype = True)\napp.run(port = 8000)\n… and then used python api.py to run the API. Once running, you can navigate to http://127.0.0.1:8000/docs in a web browser to see the autogenerated API documentation",
    "crumbs": [
      "DevOps Lessons for Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Project Architecture</span>"
    ]
  }
]